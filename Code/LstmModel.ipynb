{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvcor921DOTF"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow_text>=2.0.0rc0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "gXYlm55fUnsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/mydata.csv')"
      ],
      "metadata": {
        "id": "FGdn9R0_U68E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.dropna()\n",
        "data=data.drop('article_content',axis=1)"
      ],
      "metadata": {
        "id": "dHrSGsuFU9c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000  # Nombre maximum de mots à prendre en compte\n",
        "max_len = 100  # Longueur maximale d'une séquence\n",
        "\n",
        "# Tokenisation des articles\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data['cleandata'])\n",
        "sequences = tokenizer.texts_to_sequences(data['cleandata'])"
      ],
      "metadata": {
        "id": "o6mjDVJHd1P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(data['category']).values"
      ],
      "metadata": {
        "id": "Dnelu_qIeZnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding des séquences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Conversion des catégories en labels numériques\n",
        "labels = pd.get_dummies(data['category']).values\n",
        "train_ratio = 0.8  # Pourcentage d'articles à utiliser pour l'entraînement\n",
        "train_size = int(len(padded_sequences) * train_ratio)\n",
        "train_data = padded_sequences[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "test_data = padded_sequences[train_size:]\n",
        "test_labels = labels[train_size:]"
      ],
      "metadata": {
        "id": "jZgvhPoredb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100  # Dimension de l'espace d'incorporation des mots\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(labels.shape[1], activation='softmax'))\n"
      ],
      "metadata": {
        "id": "86oa5z4nefRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VCwKty2Me0IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "epochs = 10  # Nombre d'époques d'entraînement\n",
        "batch_size = 32  # Taille du lot\n",
        "model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Évaluation du modèle sur les données de test\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, batch_size=batch_size)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0lxxrIre_xH",
        "outputId": "b7b1c886-e6a3-4330-d84a-bd1b0fd80906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "181/181 [==============================] - 45s 231ms/step - loss: 0.5499 - accuracy: 0.7342 - val_loss: 0.2757 - val_accuracy: 0.9218\n",
            "Epoch 2/10\n",
            "181/181 [==============================] - 43s 239ms/step - loss: 0.1578 - accuracy: 0.9515 - val_loss: 0.2360 - val_accuracy: 0.9266\n",
            "Epoch 3/10\n",
            "181/181 [==============================] - 41s 229ms/step - loss: 0.0914 - accuracy: 0.9759 - val_loss: 0.2304 - val_accuracy: 0.9287\n",
            "Epoch 4/10\n",
            "181/181 [==============================] - 42s 230ms/step - loss: 0.0598 - accuracy: 0.9865 - val_loss: 0.2395 - val_accuracy: 0.9308\n",
            "Epoch 5/10\n",
            "181/181 [==============================] - 42s 234ms/step - loss: 0.0321 - accuracy: 0.9910 - val_loss: 0.2575 - val_accuracy: 0.9329\n",
            "Epoch 6/10\n",
            "181/181 [==============================] - 43s 239ms/step - loss: 0.0288 - accuracy: 0.9933 - val_loss: 0.2593 - val_accuracy: 0.9301\n",
            "Epoch 7/10\n",
            "181/181 [==============================] - 42s 230ms/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.3691 - val_accuracy: 0.9163\n",
            "Epoch 8/10\n",
            "181/181 [==============================] - 40s 222ms/step - loss: 0.0314 - accuracy: 0.9922 - val_loss: 0.3741 - val_accuracy: 0.9211\n",
            "Epoch 9/10\n",
            "181/181 [==============================] - 42s 231ms/step - loss: 0.0180 - accuracy: 0.9948 - val_loss: 0.3605 - val_accuracy: 0.9107\n",
            "Epoch 10/10\n",
            "181/181 [==============================] - 43s 236ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.4065 - val_accuracy: 0.9246\n",
            "57/57 [==============================] - 5s 79ms/step - loss: 0.4378 - accuracy: 0.9219\n",
            "Test Loss: 0.43776780366897583\n",
            "Test Accuracy: 0.9219269156455994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supposons que vous avez une nouvelle phrase en arabe à classifier\n",
        "new_text=\"القاهرة- لم يصدق عمر الشاهد ما يثار على وسائل التواصل بشأن الارتفاع الجنوني في أسعار اللحوم، وذهب بنفس رر: هل يشتري الأضحية هذا العام أم يصرف عنها النظر\"\n",
        "# Prétraitement de la nouvelle phrase\n",
        "new_sequence = tokenizer.texts_to_sequences([new_text])\n",
        "new_padded_sequence = pad_sequences(new_sequence, maxlen=max_len)\n",
        "\n",
        "# Faire la prédiction\n",
        "predictions = model.predict(new_padded_sequence)\n",
        "\n",
        "# Convertir les probabilités en catégories\n",
        "predicted_category = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Récupérer le nom de la catégorie prédite\n",
        "category_labels = data['category'].unique()\n",
        "predicted_category_label = category_labels[predicted_category[0]]\n",
        "\n",
        "print(\"Phrase :\", new_text)\n",
        "print(\"Catégorie prédite :\", predicted_category_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqASD_M-hP_T",
        "outputId": "1610b0ce-d93d-4f05-f277-2156d6426b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 664ms/step\n",
            "Phrase : القاهرة- لم يصدق عمر الشاهد ما يثار على وسائل التواصل بشأن الارتفاع الجنوني في أسعار اللحوم، وذهب بنفس رر: هل يشتري الأضحية هذا العام أم يصرف عنها النظر\n",
            "Catégorie prédite : اقتصاد\n"
          ]
        }
      ]
    }
  ]
}